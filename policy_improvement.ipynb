{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:13:06.171345Z",
     "start_time": "2020-11-29T11:13:06.168268Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.graph import *\n",
    "from src.mdp import *\n",
    "from src.policy import *\n",
    "from src.nyc import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we attempt to perform a policy improvement algorithm for our MDP. Rather than starting with a random policy (zero value functions), we start from the greedy policy we previously defined to get faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:20:37.964911Z",
     "start_time": "2020-11-29T10:20:37.962754Z"
    }
   },
   "outputs": [],
   "source": [
    "mdp = MDP['nyc']\n",
    "pi_0 = policy_greedy(mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest part of policy iteration is of course policy evaluation. In this step we must approximate for the previous policy $\\pi_{k-1}$ a value function $V_{\\pi_{k-1}}(s) \\rightarrow \\mathbb{R}$ that gives approximate expected rewards for every state. This is done in the course using the Bellman equations\n",
    "\\[V_{\\pi_{k-1}}(s) = \\sum r(s, a) + \\gamma T(s, a, s')V(s')\\]\n",
    "which you can either (1) solve directly as a system of linear equations or (2) approximate iteratively. Either approach would be daunting in this case for two reasons:\n",
    "\n",
    "* Our state space is exponentially large in the number of available drivers. In the nyc case we have approx $60^{15}$ states which is clearly too many to compute values for each\n",
    "* As this is a reinforcement learning setting, we do not know the transition function of the new rides coming in. In fact they may not even be fully Markov (dependence between one ride and the next), but this is another story."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this by approximating $V_{\\pi_{k-1}}$ with a model $\\hat V$ and fitting it on sample data. We first use a linear model, then a neural network. We make the simplification of only looking $t$ steps in the future rather than using discounting, and let successive iterations of policy improvement reach convergence. So we fit\n",
    "\n",
    "\\[V_{\\pi_{k-1}}(s) \\approx \\hat V (s)\\approx r(s, a) + \\gamma r(s', a') + \\gamma^2(r'', a'') + \\dots + \\gamma^t r(s^{(t)}, a^{(t)})\\]\n",
    "\n",
    "the RHS is approximately $r(s, a) + \\gamma T(s, a, s')V(s')$, where $[s, a, s', a', \\dots]$ is an episode we experience following policy $\\pi_{k-1}$. We experiment with different values of $t$ to see what leads to good enough convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the training set (s, V(s)) for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:10:46.079223Z",
     "start_time": "2020-11-29T11:10:46.071736Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_training_set(policy, mdp, no_iter=1000, no_ep=50, t=3, gamma=0.9):\n",
    "    '''Generates a set of samples (s, V(s)) for the given policy\n",
    "    no_iter: how many s in the training set\n",
    "    no_episodes: how many episodes to run for each s to approximate V(s)\n",
    "    t: lookforward window in the future\n",
    "    '''\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(no_iter):\n",
    "        s = mdp.get_starting_state()\n",
    "        avg_rw = 0\n",
    "        \n",
    "        for __ in range(no_ep):\n",
    "            # Generate an episode and sum up the discounted rewards\n",
    "            cur_s = s\n",
    "            rw = 0\n",
    "            for ____ in range(t):\n",
    "                a = policy(cur_s)\n",
    "                rw = rw * gamma + mdp.R(cur_s, a)\n",
    "                cur_s = mdp.T(cur_s, a)\n",
    "            avg_rw += rw / no_ep\n",
    "        \n",
    "        samples.append((s, avg_rw))\n",
    "    return samples\n",
    "\n",
    "# For details on our efficient representation of a state as a (N,) shaped array,\n",
    "#     please consult src/mdp.py\n",
    "\n",
    "def get_arrays(policy, mdp, no_iter=1000, no_ep=10, t=3, gamma=0.9):\n",
    "    \n",
    "    samples = get_training_set(policy, mdp, no_iter=no_iter, \n",
    "                               no_ep=no_ep, t=t, gamma=gamma)\n",
    "    y = np.array([s[1] for s in samples])\n",
    "    X = np.vstack([s[0].to_array(mdp) for s in samples])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:29:27.819979Z",
     "start_time": "2020-11-29T10:27:43.448521Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = get_arrays(pi_0, mdp, 100000)\n",
    "X_ts, y_ts = get_arrays(pi_0, mdp, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:29:27.824661Z",
     "start_time": "2020-11-29T10:29:27.821542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100000, 64), (100000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:31:33.661731Z",
     "start_time": "2020-11-29T10:31:33.429979Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to data folder for later use\n",
    "for st, ar in [('X', X), ('y', y), ('X_ts', X_ts), ('y_ts', y_ts)]:\n",
    "    np.save('data/'+st+'.npy', ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting estimators and generating the new policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:09:55.972202Z",
     "start_time": "2020-11-29T12:09:55.936324Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading training and test data from file\n",
    "X = np.load('data/X.npy')\n",
    "y = np.load('data/y.npy')\n",
    "X_ts = np.load('data/X_ts.npy')\n",
    "y_ts = np.load('data/y_ts.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T10:34:59.517682Z",
     "start_time": "2020-11-29T10:34:59.187613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.3002199162223307\n",
      "Test R2:  0.255204547161746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Try how well a Linear model is doing\n",
    "m = LinearRegression()\n",
    "print(\"Train R2:\", m.fit(X, y).score(X, y))\n",
    "print(\"Test R2: \", m.fit(X, y).score(X_ts, y_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I like this, not overfitting too badly, much better performance than random\n",
    "#   this is great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:17:05.294965Z",
     "start_time": "2020-11-29T12:15:15.575073Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(30, 20, 15), learning_rate='constant',\n",
       "             learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "             n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a Neural Network instead\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "scale_X = StandardScaler().fit(X)\n",
    "X_s = scale_X.transform(X)\n",
    "X_s_ts = scale_X.transform(X_ts)\n",
    "\n",
    "n = MLPRegressor(hidden_layer_sizes=(30, 20, 15), activation=\"relu\", max_iter=200)\n",
    "n.fit(X_s, -y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:17:05.418112Z",
     "start_time": "2020-11-29T12:17:05.297643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.36368992958561164\n",
      "Test R2:  0.2698395935997815\n"
     ]
    }
   ],
   "source": [
    "print(\"Train R2:\", n.score(X_s, -y))\n",
    "print(\"Test R2: \", n.score(X_s_ts, -y_ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better! But need to do more hyperparameter tuning..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:20:24.291970Z",
     "start_time": "2020-11-29T12:20:24.285390Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_Vhat(estimator, mdp):\n",
    "    def Vhat(s):\n",
    "        '''Return approximation of V by the estimator'''\n",
    "        return estimator.predict(s.to_array(mdp).reshape(1, -1))[0]\n",
    "    return Vhat\n",
    "\n",
    "def gen_Vhat_nn(net, scaler, mdp):\n",
    "    def Vhat(s):\n",
    "        X_s = scaler.transform(s.to_array(mdp).reshape(1, -1))\n",
    "        return -net.predict(X_s)[0]\n",
    "    return Vhat\n",
    "\n",
    "def policy_improvement(pi, Vhat, mdp, gamma=0.9):\n",
    "    '''Return a policy that selects a that maximizes r(s, a) + gamma Vhat(s')'''\n",
    "    def C(s, a):\n",
    "        #print(\"Wd be\", mdp.C(s, a), \" now is \", mdp.C(s, a) - gamma * Vhat(mdp.T(s, a)))\n",
    "        return mdp.C(s, a) - gamma * Vhat(mdp.T(s, a))\n",
    "    def fn(s):\n",
    "        mincost = 9999999\n",
    "        besta = None\n",
    "        for a in range(s.K):\n",
    "            cs = C(s, a)\n",
    "            if cs < mincost:\n",
    "                mincost = cs\n",
    "                besta = a\n",
    "        return besta\n",
    "    return Policy(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:01:38.107705Z",
     "start_time": "2020-11-29T11:01:37.927902Z"
    }
   },
   "outputs": [],
   "source": [
    "Vhat_0 = gen_Vhat(LinearRegression().fit(X, y), mdp)\n",
    "pi_1 = policy_improvement(pi_0, Vhat_0, mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the new pi_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:01:39.175523Z",
     "start_time": "2020-11-29T11:01:39.163927Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have a new pi_1 which mostly predicts the same things as pi_0 = greedy_policy\n",
    "s = mdp.get_starting_state()\n",
    "pi_1(s), pi_0(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:01:41.586556Z",
     "start_time": "2020-11-29T11:01:40.920462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 100 tries, pi_1(s) != pi_0(s) in 26 of them\n"
     ]
    }
   ],
   "source": [
    "tries = 100\n",
    "different = 0\n",
    "for _ in range(tries):\n",
    "    s = mdp.get_starting_state()\n",
    "    if pi_1(s) != pi_0(s):\n",
    "        different += 1\n",
    "print(\"Out of\", tries, \"tries, pi_1(s) != pi_0(s) in\", different, \"of them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:08:51.053718Z",
     "start_time": "2020-11-29T11:01:45.355207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_0 avg waiting time (loss): 38.9 pct trip time\n",
      "pi_0 avg waiting time (loss): 36.18 pct trip time\n"
     ]
    }
   ],
   "source": [
    "no_eval = 50000\n",
    "print(\"pi_0 avg waiting time (loss):\", round(mdp.eval(pi_0, no_iter=10*no_eval, return_pct_trip=True), 2), \"pct trip time\")\n",
    "print(\"pi_1 avg waiting time (loss):\", round(mdp.eval(pi_1, no_iter=no_eval, return_pct_trip=True), 2), \"pct trip time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With a NN instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:21:19.570096Z",
     "start_time": "2020-11-29T12:21:19.567645Z"
    }
   },
   "outputs": [],
   "source": [
    "Vhat_0_nn = gen_Vhat_nn(n, scale_X, mdp)\n",
    "pi_1_nn = policy_improvement(pi_0, Vhat_0_nn, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:24:40.450349Z",
     "start_time": "2020-11-29T12:23:48.082253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi_1_nn avg waiting time (loss): 29.15 pct trip time\n"
     ]
    }
   ],
   "source": [
    "no_eval = 5000\n",
    "print(\"pi_1_nn avg waiting time (loss):\", round(mdp.eval(pi_1_nn, no_iter=no_eval, return_pct_trip=True), 2), \"pct trip time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting this all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:17:46.019397Z",
     "start_time": "2020-11-29T11:17:46.015301Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_new_pi(pi, mdp, estimator, gamma=0.9,\n",
    "               no_iter=1000, no_ep=10, t=3):\n",
    "    '''Generates pi_k+1 from pi_k using policy improvement.\n",
    "    Other parameters are used to generate the training set for the estimator.\n",
    "    Please make sure your estimator does not have warm_start=True.\n",
    "    '''\n",
    "    \n",
    "    X, y = get_arrays(pi, mdp, no_iter=no_iter, no_ep=no_ep, t=t, gamma=gamma)\n",
    "    model = sklearn.base.clone(estimator).fit(X, y)\n",
    "    Vh = gen_Vhat(model, mdp=mdp)\n",
    "    return policy_improvement(pi, Vh, mdp=mdp, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T11:56:37.969544Z",
     "start_time": "2020-11-29T11:47:01.436519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Try this out multiple times starting from random policy\n",
    "mdp = MDP['nyc']\n",
    "pi = {0: policy_random}\n",
    "ks = 10\n",
    "\n",
    "for k in range(ks):\n",
    "    pi[k + 1] = get_new_pi(pi[k], mdp, LinearRegression(), no_iter=5000, no_ep=1, t=1)\n",
    "    #print(\"Iteration done\")\n",
    "\n",
    "print(\"Evaluating\")\n",
    "evals = [mdp.eval(pi[k], no_iter=3000, return_pct_trip=True) for k in range(ks+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:01:47.056164Z",
     "start_time": "2020-11-29T12:01:46.929784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lb = range(5)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.bar(lb, evals[:5], color='#557f2d')\n",
    "plt.xlabel(\"Iteration no\")\n",
    "plt.ylabel(\"Loss (in pct trip time)\")\n",
    "plt.title(\"Successive runs of policy iteration, random starting policy\")\n",
    "\n",
    "plt.xticks(lb, lb)\n",
    "\n",
    "plt.savefig(\"img/iterations.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
